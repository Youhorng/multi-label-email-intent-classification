{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-label Email Classification Fine-Tuning Task on BERT Variants**\n",
    "\n",
    "**Objective:** The goal of this task is to address the challenge of disorganized email inboxes, where multiple categories of emails are mixed together. Users often have to go through each email individually, making it difficult to quickly identify their type or priority. By fine-tuning BERT variants for multi-label classification, we aim to automatically categorize emails into their respective classes, improving inbox organization and user efficiency.\n",
    "\n",
    "**Implementation Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Project Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, hamming_loss\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the multi-label emails dataset\n",
    "dataset = load_dataset(\"imnim/multiclass-email-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: \n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['subject', 'body', 'labels'],\n",
      "        num_rows: 2105\n",
      "    })\n",
      "})\n",
      "==================================================\n",
      "Dataset Shape: Rows-2105, Columns-3\n",
      "==================================================\n",
      "Sample Data: \n",
      " {'subject': 'Meeting Reminder: Quarterly Sales Review Tomorrow', 'body': 'Dear Team, Just a friendly reminder that our Quarterly Sales Review meeting is scheduled for tomorrow at 10:00 AM in the conference room. Please make sure to bring your sales reports and any relevant updates. Coffee and pastries will be provided. Looking forward to a productive meeting. Best regards, [Your Name]', 'labels': ['Business', 'Reminders']}\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset\n",
    "print(\"Dataset: \\n\", dataset)\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset Shape: Rows-{dataset['train'].num_rows}, Columns-{len(dataset['train'].column_names)}\")\n",
    "print(\"=\"*50)\n",
    "print(\"Sample Data: \\n\", dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine subject and body of each email into a single text field\n",
    "def combine_text(examples):\n",
    "    examples[\"text\"] = examples[\"subject\"] + \" \" + examples[\"body\"]\n",
    "    return examples\n",
    "\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset = dataset.map(combine_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Meeting Reminder: Quarterly Sales Review Tomorrow',\n",
       " 'body': 'Dear Team, Just a friendly reminder that our Quarterly Sales Review meeting is scheduled for tomorrow at 10:00 AM in the conference room. Please make sure to bring your sales reports and any relevant updates. Coffee and pastries will be provided. Looking forward to a productive meeting. Best regards, [Your Name]',\n",
       " 'labels': ['Business', 'Reminders'],\n",
       " 'text': 'Meeting Reminder: Quarterly Sales Review Tomorrow Dear Team, Just a friendly reminder that our Quarterly Sales Review meeting is scheduled for tomorrow at 10:00 AM in the conference room. Please make sure to bring your sales reports and any relevant updates. Coffee and pastries will be provided. Looking forward to a productive meeting. Best regards, [Your Name]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the updated dataset\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset before further processing\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode multi-labels using MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_dataset[\"labels\"])\n",
    "num_labels = len(mlb.classes_)\n",
    "\n",
    "# Define a function to encode the labels\n",
    "def encode_labels(examples):\n",
    "    # Transform the entire list of labels for each sample\n",
    "    encoded = mlb.transform(examples[\"labels\"])\n",
    "    # Convert to float32 (important for multi-label)\n",
    "    examples[\"labels_encoded\"] = encoded.astype(np.float32).tolist()\n",
    "    return examples\n",
    "\n",
    "# Apply the function to the dataset\n",
    "train_dataset = train_dataset.map(encode_labels, batched=True)\n",
    "test_dataset = test_dataset.map(encode_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: [['Business', 'Reminders'], ['Promotions']]\n",
      "Encoded labels: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]]\n",
      "Label classes: ['Business' 'Customer Support' 'Events & Invitations' 'Finance & Bills'\n",
      " 'Job Application' 'Newsletters' 'Personal' 'Promotions' 'Reminders'\n",
      " 'Travel & Bookings']\n"
     ]
    }
   ],
   "source": [
    "# Check the encoded dataset\n",
    "print(\"Original labels:\", train_dataset[\"labels\"][:2])\n",
    "print(\"Encoded labels:\", train_dataset[\"labels_encoded\"][:2])\n",
    "print(\"Label classes:\", mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilt datasets successfully!\n",
      "Train columns: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize and encode from your original data\n",
    "def tokenize_and_encode(examples):\n",
    "    # Tokenize text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        truncation=True, \n",
    "        max_length=256,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Encode labels\n",
    "    encoded_labels = mlb.transform(examples[\"labels\"])\n",
    "    tokenized[\"labels\"] = encoded_labels.astype(np.float32).tolist()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Apply to clean datasets \n",
    "train_dataset = train_dataset.map(tokenize_and_encode, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_and_encode, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train_dataset = train_dataset.remove_columns([\"subject\", \"body\", \"text\", \"labels_encoded\"])\n",
    "test_dataset = test_dataset.remove_columns([\"subject\", \"body\", \"text\", \"labels_encoded\"])\n",
    "\n",
    "print(\"Rebuilt datasets successfully!\")\n",
    "print(\"Train columns:\", train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample: {'labels': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], 'input_ids': [101, 14764, 1024, 9046, 2136, 3116, 6203, 2136, 2372, 1010, 2023, 2003, 1037, 5379, 14764, 2008, 2057, 2031, 2256, 4882, 2136, 3116, 5115, 2005, 4826, 2012, 2184, 1024, 4002, 2572, 1012, 3531, 2191, 2469, 2000, 3319, 1996, 11376, 25828, 1998, 2272, 4810, 2007, 2151, 14409, 2030, 20062, 2000, 3745, 1012, 2292, 1005, 1055, 2031, 1037, 13318, 3116, 1998, 6848, 2256, 5082, 2006, 7552, 3934, 1012, 2559, 2830, 2000, 3773, 2017, 2035, 2045, 999, 2190, 12362, 1010, 1031, 2115, 2171, 1033, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Verify the structure\n",
    "print(\"Training sample:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID to Label: {0: 'Business', 1: 'Customer Support', 2: 'Events & Invitations', 3: 'Finance & Bills', 4: 'Job Application', 5: 'Newsletters', 6: 'Personal', 7: 'Promotions', 8: 'Reminders', 9: 'Travel & Bookings'}\n",
      "Label to ID: {'Business': 0, 'Customer Support': 1, 'Events & Invitations': 2, 'Finance & Bills': 3, 'Job Application': 4, 'Newsletters': 5, 'Personal': 6, 'Promotions': 7, 'Reminders': 8, 'Travel & Bookings': 9}\n"
     ]
    }
   ],
   "source": [
    "# Prepare the label ids\n",
    "id2label = {i: label for i, label in enumerate(mlb.classes_)}\n",
    "label2id = {label: i for i, label in enumerate(mlb.classes_)}\n",
    "\n",
    "# Check the mappings\n",
    "print(\"ID to Label:\", id2label)\n",
    "print(\"Label to ID:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,           # Enable padding\n",
    "    return_tensors=\"pt\",    # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear memory\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DistilBERT model for fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with 10 labels: ['Business', 'Customer Support', 'Events & Invitations', 'Finance & Bills', 'Job Application', 'Newsletters', 'Personal', 'Promotions', 'Reminders', 'Travel & Bookings']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "print(\"Loading DistilBERT model for fine-tuning...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "print(f\"Model loaded with {num_labels} labels: {list(mlb.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration applied to the model.\n",
      "trainable params: 1,188,106 || all params: 68,149,268 || trainable%: 1.7434\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for parameter-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"LoRA configuration applied to the model.\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=10, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=10, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device training on MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation logic for multi-label classification\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to convert logits to probabilities\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    \n",
    "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    \n",
    "    # Ensure labels are in correct format\n",
    "    y_true = labels\n",
    "    \n",
    "    # Calculate various multi-label metrics\n",
    "    \n",
    "    # Exact match: all labels must be predicted correctly\n",
    "    exact_match = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Hamming loss: fraction of incorrectly predicted labels\n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    # Micro-averaged metrics (global across all labels)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='micro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro-averaged metrics (average per label)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Calculate label-wise accuracy (useful for imbalanced datasets)\n",
    "    label_accuracy = np.mean((y_true == y_pred).all(axis=1))\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match,\n",
    "        'label_accuracy': label_accuracy,\n",
    "        'hamming_loss': hamming,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'recall_macro': recall_macro,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured\n"
     ]
    }
   ],
   "source": [
    "# Setup the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=\"multi-label-email-classification-finetuning/distilbert-lora-multi-label\",\n",
    "    logging_dir=\"multi-label-email-classification-finetuning/logs\",\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,      # Small batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,       # Can be larger for evaluation\n",
    "    gradient_accumulation_steps=4,       # Simulates batch size of 16 (4*4)\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=5e-4,                 # Higher learning rate for LoRA\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Evaluation and saving (UPDATED NAMES)\n",
    "    eval_strategy=\"steps\",              # Changed from evaluation_strategy\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_micro\",  # Added 'eval_' prefix\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Memory optimization (especially for Apple Silicon)\n",
    "    dataloader_pin_memory=False,        # Disable for MPS\n",
    "    dataloader_num_workers=0,           # Best for MPS\n",
    "    gradient_checkpointing=True,        # Trade speed for memory\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=None,  # Disable wandb/tensorboard if not needed\n",
    "    \n",
    "    # Early stopping patience\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully\n",
      "Training samples: 1684\n",
      "Evaluation samples: 421\n"
     ]
    }
   ],
   "source": [
    "#  Create the Trainer with all components\n",
    "trainer = Trainer(\n",
    "    model=model,                        # LoRA-adapted model\n",
    "    args=training_args,                 # Training configuration\n",
    "    train_dataset=train_dataset,        # Training data\n",
    "    eval_dataset=test_dataset,          # Evaluation data\n",
    "    processing_class=tokenizer,         # Tokenizer for text processing\n",
    "    data_collator=data_collator,        # Dynamic padding\n",
    "    compute_metrics=compute_metrics,    # Evaluation metrics\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MODEL INFORMATION\n",
      "==================================================\n",
      "Total parameters: 68,149,268\n",
      "trainable params: 1,188,106 || all params: 68,149,268 || trainable%: 1.7434\n"
     ]
    }
   ],
   "source": [
    "#  Display model info before training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total parameters: {model.num_parameters():,}\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING TRAINING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='318' max='318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [318/318 00:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match Accuracy</th>\n",
       "      <th>Label Accuracy</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.232300</td>\n",
       "      <td>0.236745</td>\n",
       "      <td>0.437055</td>\n",
       "      <td>0.437055</td>\n",
       "      <td>0.095249</td>\n",
       "      <td>0.663308</td>\n",
       "      <td>0.481088</td>\n",
       "      <td>0.768482</td>\n",
       "      <td>0.647561</td>\n",
       "      <td>0.583456</td>\n",
       "      <td>0.470470</td>\n",
       "      <td>2.671000</td>\n",
       "      <td>157.620000</td>\n",
       "      <td>19.843000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully!\n",
      "Final training loss: 0.2934\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    training_results = trainer.train()\n",
    "    \n",
    "    print(\"Training completed successfully!\")\n",
    "    print(f\"Final training loss: {training_results.training_loss:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    # If memory error, suggest reducing batch size\n",
    "    if \"memory\" in str(e).lower():\n",
    "        print(\"Suggestion: Reduce per_device_train_batch_size to 2 or 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Label Classification Results:\n",
      "----------------------------------------\n",
      "Loss                     : 0.2367\n",
      "Exact Match Accuracy     : 0.4371\n",
      "Label Accuracy           : 0.4371\n",
      "Hamming Loss             : 0.0952\n",
      "F1 Micro                 : 0.6633\n",
      "F1 Macro                 : 0.4811\n",
      "Precision Micro          : 0.7685\n",
      "Precision Macro          : 0.6476\n",
      "Recall Micro             : 0.5835\n",
      "Recall Macro             : 0.4705\n",
      "Runtime                  : 2.3773\n",
      "Samples Per Second       : 177.0940\n",
      "Steps Per Second         : 22.2940\n",
      "\n",
      "==============================\n",
      "PER-LABEL ANALYSIS\n",
      "==============================\n",
      "Business       : P=0.777, R=0.812, F1=0.794, Support=176.0\n",
      "Customer Support: P=1.000, R=0.024, F1=0.047, Support=42.0\n",
      "Events & Invitations: P=0.789, R=0.563, F1=0.657, Support=126.0\n",
      "Finance & Bills: P=0.887, R=0.829, F1=0.857, Support=76.0\n",
      "Job Application: P=0.000, R=0.000, F1=0.000, Support=30.0\n",
      "Newsletters    : P=0.667, R=0.067, F1=0.121, Support=30.0\n",
      "Personal       : P=0.000, R=0.000, F1=0.000, Support=59.0\n",
      "Promotions     : P=0.889, R=0.696, F1=0.780, Support=23.0\n",
      "Reminders      : P=0.467, R=0.764, F1=0.579, Support=55.0\n",
      "Travel & Bookings: P=1.000, R=0.950, F1=0.974, Support=60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/youhorng/Desktop/projects/multi-label-email-intent-classification/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# 11.1: Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# 11.2: Display results in a formatted way\n",
    "print(\"Multi-Label Classification Results:\")\n",
    "print(\"-\" * 40)\n",
    "for metric, value in eval_results.items():\n",
    "    if metric.startswith('eval_'):\n",
    "        metric_name = metric.replace('eval_', '').replace('_', ' ').title()\n",
    "        print(f\"{metric_name:<25}: {value:.4f}\")\n",
    "\n",
    "# 11.3: Analyze per-label performance\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"PER-LABEL ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred_probs = torch.sigmoid(torch.tensor(predictions.predictions))\n",
    "y_pred = (y_pred_probs >= 0.5).int().numpy()\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Calculate per-label metrics\n",
    "for i, label_name in enumerate(mlb.classes_):\n",
    "    label_true = y_true[:, i]\n",
    "    label_pred = y_pred[:, i]\n",
    "    \n",
    "    # Skip if no positive samples\n",
    "    if label_true.sum() == 0:\n",
    "        continue\n",
    "        \n",
    "    precision = precision_recall_fscore_support(label_true, label_pred, average='binary')[0]\n",
    "    recall = precision_recall_fscore_support(label_true, label_pred, average='binary')[1]\n",
    "    f1 = precision_recall_fscore_support(label_true, label_pred, average='binary')[2]\n",
    "    support = label_true.sum()\n",
    "    \n",
    "    print(f\"{label_name:<15}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}, Support={support}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "TESTING PREDICTIONS\n",
      "==============================\n",
      "\n",
      "Test Email 1:\n",
      "Text: Reminder: Team meeting tomorrow at 10 AM in conference room....\n",
      "Predicted Labels: ['Business', 'Reminders']\n",
      "Top Probabilities:\n",
      "  Business: 0.792\n",
      "  Reminders: 0.776\n",
      "  Events & Invitations: 0.256\n",
      "\n",
      "Test Email 2:\n",
      "Text: 50% off all items this weekend! Don't miss this amazing deal...\n",
      "Predicted Labels: ['Promotions']\n",
      "Top Probabilities:\n",
      "  Promotions: 0.553\n",
      "  Newsletters: 0.396\n",
      "  Events & Invitations: 0.128\n",
      "\n",
      "Test Email 3:\n",
      "Text: Your account statement for this month is ready. Please revie...\n",
      "Predicted Labels: []\n",
      "Top Probabilities:\n",
      "  Travel & Bookings: 0.386\n",
      "  Finance & Bills: 0.299\n",
      "  Customer Support: 0.188\n",
      "\n",
      "Test Email 4:\n",
      "Text: Breaking: Major tech company announces new product launch. S...\n",
      "Predicted Labels: ['Business', 'Events & Invitations']\n",
      "Top Probabilities:\n",
      "  Business: 0.750\n",
      "  Events & Invitations: 0.627\n",
      "  Newsletters: 0.447\n"
     ]
    }
   ],
   "source": [
    "# # 12.1: Save the fine-tuned model\n",
    "# print(\"\\n\" + \"=\"*30)\n",
    "# print(\"SAVING MODEL\")\n",
    "# print(\"=\"*30)\n",
    "\n",
    "# save_path = \"./final-distilbert-lora-multi-label\"\n",
    "# trainer.save_model(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# 12.2: Create prediction function for new texts\n",
    "def predict_email_labels(text, threshold=0.5, return_probabilities=False):\n",
    "    \"\"\"\n",
    "    Predict labels for a new email text\n",
    "    \n",
    "    Args:\n",
    "        text (str): Email text to classify\n",
    "        threshold (float): Probability threshold for prediction\n",
    "        return_probabilities (bool): Whether to return probabilities\n",
    "    \n",
    "    Returns:\n",
    "        list: Predicted labels\n",
    "        dict (optional): Label probabilities if return_probabilities=True\n",
    "    \"\"\"\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=256,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Apply sigmoid to get probabilities\n",
    "        probabilities = torch.sigmoid(outputs.logits)[0]\n",
    "    \n",
    "    # Convert to numpy for easier handling\n",
    "    probs = probabilities.cpu().numpy()\n",
    "    \n",
    "    # Get predicted labels based on threshold\n",
    "    predicted_labels = []\n",
    "    label_probs = {}\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        label_name = mlb.classes_[i]\n",
    "        label_probs[label_name] = float(prob)\n",
    "        \n",
    "        if prob >= threshold:\n",
    "            predicted_labels.append(label_name)\n",
    "    \n",
    "    if return_probabilities:\n",
    "        return predicted_labels, label_probs\n",
    "    else:\n",
    "        return predicted_labels\n",
    "\n",
    "# 12.3: Test the prediction function\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"TESTING PREDICTIONS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Test with sample emails\n",
    "test_emails = [\n",
    "    \"Reminder: Team meeting tomorrow at 10 AM in conference room. Please bring your reports.\",\n",
    "    \"50% off all items this weekend! Don't miss this amazing deal. Shop now!\",\n",
    "    \"Your account statement for this month is ready. Please review the attached document.\",\n",
    "    \"Breaking: Major tech company announces new product launch. Stock prices surge.\"\n",
    "]\n",
    "\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    print(f\"\\nTest Email {i}:\")\n",
    "    print(f\"Text: {email[:60]}...\")\n",
    "    \n",
    "    labels, probs = predict_email_labels(email, return_probabilities=True)\n",
    "    \n",
    "    print(f\"Predicted Labels: {labels}\")\n",
    "    print(\"Top Probabilities:\")\n",
    "    sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    for label, prob in sorted_probs:\n",
    "        print(f\"  {label}: {prob:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-label-email-intent-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
